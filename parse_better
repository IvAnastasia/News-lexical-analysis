! pip install beautifulsoup4

! pip install requests

import requests
import re
from bs4 import BeautifulSoup

def parse(link):
    if 'https://meduza' in link:
        return meduza(link)
    if 'novayagazeta.ru' in link:
        return novaya(link)
    if 'https://tvrain' in link:
        return rain(link)
    if 'www.rbc' in link:
        return rbc(link)
    if 'tsargrad.tv' in link:
        return tsar(link)
    if 'https://ria.' in link:
        return ria(link)
    if 'russian.rt' in link:
        return rt(link)
    if 'rg.ru' in link:
        return rg(link)
    

def meduza(link):
    page = requests.get(link)
    soup = BeautifulSoup(page.text, "html.parser")
    alltext = soup.findAll('p')
    newstext = soup.title.text + '. '
    for data in alltext:
        newstext = newstext + data.text + ' '
    newstext = newstext.replace(' — Meduza', '')
    newstext = text.replace('Данное сообщение (материал) создано и (или) распространено иностранным средством массовой информации, выполняющим функции иностранного агента, и (или) российским юридическим лицом, выполняющим функции иностранного агента.  ', '')
    return newstext + '\n'

def novaya(link):
    pass
    #we have troubles here
    
def rain(link):
    reg = re.compile(r"Вы уже подписчик[\s*\S*]*")
    page = requests.get(link)
    soup = BeautifulSoup(page.text, "html.parser")
    alltext = soup.findAll('p')
    newstext = soup.title.text + '. '
    for data in alltext:
        newstext = newstext + data.text + ' '
    newstext = re.sub(reg, '', newstext)
    return newstext + '\n'

def rbc(link):
    reg = re.compile(r" :: .+ :: РБК")
    page = requests.get(link)
    soup = BeautifulSoup(page.text, "html.parser")
    alltext = soup.findAll('p')
    newstext = soup.title.text + '. '
    for data in alltext:
        if data.find(class_='article__inline-item__title') is not None:
            pass
        else:
            newstext = newstext + data.text + ' '
    newstext = re.sub(reg, '', newstext)
    return newstext + '\n'

def tsar(link):
    reg = re.compile(r"\s{1,}Средство массовой информации сетевое издание[\s*\S*]*")
    page = requests.get(link)
    soup = BeautifulSoup(page.text, "html.parser")
    alltext = soup.findAll('p')
    newstext = soup.title.text + '. '
    for data in alltext:
        newstext = newstext + data.text + ' '
    newstext = re.sub(reg, '', newstext)
    return newstext + '\n'

def ria(link):
    pass #now

def rt(link):
    reg = re.compile(r"© Автономная некоммерческая организация.*")
    page = requests.get(link)
    soup = BeautifulSoup(page.text, "html.parser")
    alltext = soup.findAll('p')
    newstext = soup.title.text + '. '
    for data in alltext:
        newstext = newstext + data.text + ' '
    newstext = re.sub(reg, '', newstext)
    newstext = newstext.replace(' — РТ на русском', '')
    return newstext + '\n'
    
def rg(link):
    page = requests.get(link)
    soup = BeautifulSoup(page.text, "html.parser")
    alltext = soup.findAll('p')
    newstext = soup.title.text + '. '
    newstext = newstext.replace(' — Российская газета', '') 
    for data in alltext:
        newstext = newstext + data.text
    return newstext + '\n'

#linksdata

for n in range(len(rus_li)):
    for li in rus_links:
        with open('rusnew' + str(n) + '.txt', 'a') as file:
            for link in li:
                newstext = parse(link)
                file.write(newstext)
        with open('rusnew' + str(n) + '.txt', 'r') as file1:
            text = file1.read()
            with open('rusnews.txt', 'a') as file2:
                file2.write(text)
    for li in indep_links:
        with open('indepnew' + str(n) + '.txt', 'a') as file:
            for link in li:
                newstext = parse(link)
                file.write(newstext)
        with open('indepnew' + str(n) + '.txt', 'r') as file1:
            text = file1.read()
            with open('indepnews.txt', 'a') as file2:
                file2.write(text)
                
                
                
                
reg = re.compile(r" - .+2021")
page = requests.get('https://ria.ru/20210514/er-1732313789.html')
soup = BeautifulSoup(page.text, "html.parser")
newstext = ''
alltext = soup.findAll('div')
newstext = soup.title.text + '. '
for data in alltext:
    if data.find(itemprop_ = "articleBody") is True:
        print(data)
        #   newstext = newstext + data.text + ' '
newstext = re.sub(reg, '', newstext)
print(newstext)
